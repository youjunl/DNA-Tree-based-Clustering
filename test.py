# -*- coding: utf-8 -*-
"""Starcode Improvement

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_dIn28vR-yiHppl1awAxAp9HVGVH8TTq
"""

import random
import time
import numpy as np
import matplotlib.pyplot as plt
import cupy
from collections import defaultdict
from queue import PriorityQueue
from tqdm import tqdm

"""#Dataset Parameter



[Prove](https://www.askamathematician.com/2010/07/q-whats-the-chance-of-getting-a-run-of-k-successes-in-n-bernoulli-trials-why-use-approximations-when-the-exact-answer-is-known/)
"""

pi = 0.01
pd = 0.01
ps = 0.01
# Number of Bernoulli trie
n = 14
# Numbrt of extra length
m = 16
# Number of required streak
k = 2
# Number of clusters
nClust = 200
# Number of sampling
ns = 200
# Probability of successful sampling
pss = 1
# Shuffuling
shuffle = True

simulations = 100000
gamma_step = 0.001
gamma = [g for g in np.arange(0, 1 + gamma_step, gamma_step)]

"""# Create a virtual dataset"""

tofile = 'data.txt'
dna_to_num = {'A': 0, 'T': 1, 'G': 2, 'C': 3}
num_to_dna = {0: 'A', 1: 'T', 2: 'G', 3: 'C'}
num_to_dna_array = ['A', 'T', 'G', 'C']


def channel(txSeqs, pi, pd, ps):
    rxSeqs = []
    simErr = 0
    for ind, tx in enumerate(txSeqs):
        errorFlag = ''
        i, rx = 0, ''
        while i < len(tx):
            if randsel(pi):  # Insertion
                rx += num_to_dna_array[random.randint(0, 3)]
                errorFlag += 'e'
                continue
            if not randsel(pd):  # Deletion
                if randsel(ps):  # Substitution
                    tmp = num_to_dna_array.copy()
                    tmp.remove(tx[i])
                    rx += tmp[random.randint(0, 2)]
                    errorFlag += 'e'
                else:
                    rx += tx[i]
                    errorFlag += '_'
            else:
                errorFlag += 'e'
            i += 1
        rxSeqs.append(rx)
        if 'ee' in errorFlag[:n]:
            simErr += 1

    return rxSeqs, simErr


def randsel(p):
    sel = np.random.rand() < p
    return sel


txs = []
clInds = []
refIndex = []
for i in range(nClust):
    # Generate a random data sequence
    tx = ''
    for _ in range(n + m):
        tx += num_to_dna[random.randint(0, 3)]
    refIndex.append(tx[:n])
    # Generate number of sampling
    nSample = np.random.binomial(ns, pss)
    txs.extend([tx for _ in range(nSample)])
    clInds.extend([i + 1 for _ in range(nSample)])
# Generate noisy data sequences
rxs, simErr = channel(txs, pi, pd, ps)
if shuffle:
    rxs = np.array(rxs)
    clInds = np.array(clInds)
    inds = np.arange(len(rxs))
    np.random.shuffle(inds)
    rxs = rxs[inds]
    clInds = clInds[inds]
# Write data
with open(tofile, 'w') as f:
    f.truncate(0)
    for ind, rx in zip(clInds, rxs):
        f.write(str(ind) + ' ' + rx + '\n')

with open('refInd.txt', 'w') as f:
    f.truncate(0)
    for ind in clInds:
        f.write(str(ind) + '\n')

with open('refIndex.txt', 'w') as f:
    f.truncate(0)
    for index in refIndex:
        f.write(index + '\n')

"""# Clustering Alg"""


class Node:
    def __init__(self) -> None:
        self.node_num = 4
        self.children = [None for _ in range(self.node_num)]
        self.isEnd = False
        self.singleBranch = True


class QueueItem:
    def __init__(self, data, error=[0, 0, 0]) -> None:
        self.data = data
        self.error = error

    def __lt__(self, other):
        return sum(self.error) <= sum(other.error)


class Trie:
    def __init__(self):
        self.dna_dict = {"A": 0, "T": 1, "G": 2, "C": 3}
        self.dna_dict_inv = {0: "A", 1: "T", 2: "G", 3: "C"}
        self.node_nums = 4
        self.prev_num = 2
        self.maxOptimDepth = 4
        self.minTerminate = 14
        self.children = [None for _ in range(self.node_nums)]
        self.isEnd = False
        self.singleBranch = False

    def insert(self, word: str, label: str) -> None:
        node = self
        dict = self.dna_dict
        for i, ch in enumerate(word):
            ch = dict[ch]
            if not node.children[ch]:
                node.children[ch] = Node()
                node = node.children[ch]
            else:
                node = node.children[ch]
                node.singleBranch = False
        node.isEnd = label

    def delete(self, word: str):
        node = self
        dict = self.dna_dict
        for ch in word:
            ch = dict[ch]
            if not node.children[ch]:
                node.children[ch] = Trie()
            node = node.children[ch]
        node.isEnd = False

    def fuzz_align(self, word, node, depth, max_value, memory, prev=''):
        # Exceed edit threshold
        if min(memory) > max_value:
            return False

        # Traverse to the end, return the cluster index and error number
        if node.isEnd != False:
            return [node.isEnd, memory[max_value]]

        chNum = self.dna_dict[word[depth]]

        # Update memory
        if depth < max_value:
            memory[max_value - depth - 1] = depth + 1
            memory[max_value + depth + 1] = depth + 1
        inp = word[max(0, depth + 1 - max_value):depth + 1]
        # If node exist
        if node.children[chNum]:
            out = prev
            out += self.dna_dict_inv[chNum]
            new_memory = self.update_memory(memory, inp, out, max_value)
            result = self.fuzz_align(word, node.children[chNum], depth + 1, max_value, new_memory, out)
            # If traverse fail, try other nodes
            if result == False:
                for tmp_chNum, tmp_node in enumerate(node.children):
                    if tmp_node and tmp_chNum != chNum:
                        out = prev
                        out += self.dna_dict_inv[tmp_chNum]
                        new_memory = self.update_memory(memory, inp, out, max_value)
                        result = self.fuzz_align(word, tmp_node, depth + 1, max_value, new_memory, out)
                        if result:
                            return result # Find a matching branch
                return False  # Cannnot find a branch
            else:
                return result  # Found matching branch
        else:
            for tmp_chNum, tmp_node in enumerate(node.children):
                if tmp_node:
                    out = prev
                    out += self.dna_dict_inv[tmp_chNum]
                    new_memory = self.update_memory(memory, inp, out, max_value)
                    result = self.fuzz_align(word, tmp_node, depth + 1, max_value, new_memory, out)
                    if result != False:
                        return result
            return False  # Cannnot find a branch

    def update_memory(self, memory, inp, out, max_value):
        new_memory = memory.copy()
        if len(out) >= max_value:
            new_memory[0] = min(memory[0] + int(out[-1] != inp[0]), memory[1] + 1)
            new_memory[-1] = min(memory[-1] + int(out[-max_value] != inp[-1]), memory[-2] + 1)
            for i in range(1, max_value):
                delta = int(out[-1] != inp[i - 1])
                new_memory[i] = min(new_memory[i - 1] + 1, memory[i] + delta, memory[i + 1] + 1)
                j = -i - 1
                delta = int(out[-i] != inp[-1])
                new_memory[j] = min(new_memory[j + 1] + 1, memory[j] + delta, memory[j - 1] + 1)

        else:
            offset = max_value - len(out)
            for i in range(offset + 1, max_value):
                delta = int(out[-1] != inp[i - offset - 1])
                new_memory[i] = min(new_memory[i - 1] + 1, memory[i] + delta, memory[i + 1] + 1)
                j = -i - 1
                delta = int(out[i - offset - 1] != inp[-1])
                new_memory[j] = min(new_memory[j + 1] + 1, memory[j] + delta, memory[j - 1] + 1)

        delta = int(out[-1] != inp[-1])
        new_memory[max_value] = min(memory[max_value] + delta, memory[max_value - 1] + 1, memory[max_value + 1] + 1)
        return new_memory

    def fuzz_fin(self, inData, max_value, tree_depth=16):
        word = inData[:tree_depth]
        node = self
        memory = [1000 for _ in range(int(2 * max_value + 1))]
        memory[max_value] = 0
        depth = 0
        result = self.fuzz_align(word, node, depth, max_value, memory, prev='')
        if result == False:
            return ["", 1000]
        else:
            return result


def clust(tree, dnaData, indexBegin=0, tree_depth=16, tree_threshold=6):
    test_num = 0
    indexList = []
    dna_number = indexBegin
    timer = time.time()
    error = np.array([0, 0, 0])
    freqs = [0 for _ in range(len(dnaData))]
    adjacent_list = []
    core_set = dict()
    for i, seq in tqdm(enumerate(dnaData)):
        test_num += 1
        align = tree.fuzz_fin(seq, tree_threshold, tree_depth)
        if align[1] < tree_threshold:
            indexList.append((clInds[indexBegin + i], align[0]))
        else:
            dna_number += 1
            tree.insert(seq[:tree_depth], dna_number)
            core_set[dna_number] = seq
            indexList.append((clInds[indexBegin + i], dna_number))  # Fix Clover's problem
    # Output Result
    t = (time.time() - timer)
    sortedList = sorted(indexList, key=lambda k: k[0])
    prob = error / len(dnaData) / len(dnaData[0])
    return indexList


def clust_with_index(tree, dnaData, indexBegin=0, tree_depth=16, tree_threshold=6):
    dna_number = indexBegin
    with open('refIndex.txt', 'r') as f:
        for line in f.readlines():
            dna_number += 1
            seq = line.strip()
            tree.insert(seq[:tree_depth], dna_number)
    test_num = 0
    indexList = []
    for i, seq in tqdm(enumerate(dnaData)):
        test_num += 1
        align = tree.fuzz_fin(seq, tree_threshold, tree_depth)
        if align[1] < tree_threshold:
            indexList.append((clInds[indexBegin + i], align[0]))
        else:
            dna_number += 1
            indexList.append((clInds[indexBegin + i], dna_number))  # Fix Clover's problem

    return indexList


def computeAccuracy(indexList, gamma):
    f = open('refInd.txt', 'r').readlines()
    refind = [int(t.strip()) for t in f]
    algind = indexList
    # caculate freqs
    clustNum = defaultdict(int)
    results = indexList
    for ind in refind:
        clustNum[ind] += 1

    results = sorted(results, key=lambda k: k[1])
    nClust = len(clustNum.keys())

    maxClustNum = results[-1][1] if len(results) > 0 else 0
    clusters = [[] for _ in range(maxClustNum)]
    for pair in results:
        clusters[pair[1] - 1].append(pair[0])

    clusters = [c for c in clusters if c != []]
    WrongClusterNum = 0
    score = [0] * max(clustNum.keys())
    for cluster in clusters:
        # Check if all the tags in a clusters are the same.
        tags = set(cluster)
        if len(tags) > 1:
            WrongClusterNum += 1
            # This cluster is invalid
            continue
        # Maximize the size of the clusters.
        tag = int(cluster[0])
        score[tag - 1] = max(score[tag - 1], len(cluster))

    # Compute accuracy
    acc = [0] * len(gamma)
    for i, g in enumerate(gamma):
        cnt = 0
        for tag in clustNum.keys():
            if score[tag - 1] / clustNum[tag] >= g:
                cnt += 1
        acc[i] = cnt / nClust
    return acc


"""序列会优先进入到前缀更符合的序列中，深度优先，最终导致提前终止。
解决方案: 对于每个符合的序列都要进行尝试。广度优先，每走一步匹配一次

# Compute theoretical accuracy

当一个序列有两个连续的IDS错误时，它会被认为是一个新的分支。因为对于每个类来说，进行聚类是ns次伯努利试验。可以假设聚类的初始大小服从二项分布，在该二项分布中采样nClust次，每次采样后聚类的结果产生d的误差。此时引入gamma并求和
"""


def probFailure(n, k, prob, saved=None):
    # Initial a hash table to store the value
    if saved == None:
        saved = {}
    ID = (n, k, prob)
    if ID in saved:
        return saved[ID]
    else:
        if k > n or n <= 0:
            result = 0
        elif n == k:
            result = prob ** k
        else:
            result = prob ** k + (n - k) * (1 - prob) * prob ** k
            for i in range(k, n - k):
                pr = probFailure(n - i, k, prob, saved)
                result -= (1 - prob) * prob ** k * pr
            saved[ID] = result
        return result


def simFailure(n, k, prob):
    simulations = 100000
    cnt = 0
    for _ in range(simulations):
        d = [randsel(prob) for _ in range(n)]
        for i in range(0, n - 1):
            if d[i] and d[i] == d[i + 1]:
                cnt += 1
                break
    result = cnt / simulations
    return result


probErr = 1 - (1 - pi - pd) * (1 - ps)
probFailureClust = probFailure(n, k, probErr, saved=None)
print(probFailureClust)
# print(simFailure(n, k, probErr))
expFailureClust = ns * pss * probFailureClust
# Monte Carlo Simulation
simClusterCoverage = []
for _ in range(simulations):
    d = np.random.binomial(ns, pss)
    failure = np.random.binomial(d, probFailureClust)
    success = d - failure
    simClusterCoverage.append(success / d)

accMCMC = [0] * len(gamma)
simClusterCoverage = np.array(simClusterCoverage)
for i, g in enumerate(gamma):
    cnt = np.sum(simClusterCoverage >= g)
    accMCMC[i] = cnt / simulations

"""# SSC Clustering"""


def mutual_edit(X, length):
    X = one_hot_encoding(X, {'A': 0, 'T': 1, 'G': 2, 'C': 3}, length)
    X = cupy.array(X)
    n = len(X)
    I = np.ravel(np.broadcast_to(np.arange(n), (n, n)).T)
    J = np.ravel(np.broadcast_to(np.arange(n), (n, n)))
    d = edit_distance(X[I], X[J])
    d = cupy.asnumpy(d).reshape(n, n)
    return d


def all_edit(y, X, length):
    X = one_hot_encoding(X, {'A': 0, 'T': 1, 'G': 2, 'C': 3}, length)
    X = cupy.array(X)
    y = one_hot_encoding([y], {'A': 0, 'T': 1, 'G': 2, 'C': 3}, length)
    n = len(X)
    y = cupy.array([y[0] for _ in range(n)])
    d = edit_distance(y, X)
    d = cupy.asnumpy(d).reshape(n, )
    return d


def one_hot_encoding(X, dict_alphabet, max_seq_length, smooth=1.):
    out = np.zeros((len(X), len(dict_alphabet), max_seq_length), dtype=np.float32)
    if smooth < 1:
        out[:] = (1 - smooth) / (len(dict_alphabet) - 1)
    for i, seq in enumerate(X):
        l = len(seq)
        for j, c in enumerate(seq):
            out[i, dict_alphabet[c], j] = smooth
        out[i, :, l:] = 0
    return out


def edit_distance(x1, x2, l1=None, l2=None, normolized=False):
    if len(x1.shape) == 3:
        kernel = cupy.ElementwiseKernel(
            'raw T x1, raw T x2, raw Z l1, raw Z l2, Z max_l, Z n_symbol',
            'raw T d',
            """
            int offset = i * (max_l + 1) * (max_l + 1);
            for(int j = 0; j < l1[i] + 1; j++)
                d[offset + j * (max_l + 1)] = j;
            for(int k = 0; k < l2[i] + 1; k++)
                d[offset + k] = k;
            for(int j = 1; j < l1[i] + 1; j++){
                for(int k = 1; k < l2[i] + 1; k++){
                    int index = offset + j * (max_l + 1) + k;
                    T delta = 0;
                    int offset1 = i * max_l * n_symbol + j - 1;
                    int offset2 = i * max_l * n_symbol + k - 1;
                    for(int r = 0; r < n_symbol; r++)
                        delta += max(x1[offset1 + r * max_l] - x2[offset2 + r * max_l], 0.0);
                    d[index] = min(d[index - (max_l + 1)] + 1, min(d[index - 1] + 1, d[index - max_l - 2] + delta));
                }  
            } 
            """,
            name='edit_distance_kernel_W'
        )
        if l1 is None:
            l1 = cupy.sum((cupy.sum(x1, axis=1) > 0).astype(np.int32), axis=1)
            l2 = cupy.sum((cupy.sum(x2, axis=1) > 0).astype(np.int32), axis=1)
        d = cupy.zeros((x1.shape[0], x1.shape[2] + 1, x1.shape[2] + 1), dtype=np.float32)
        kernel(x1, x2, l1, l2, x1.shape[2], x1.shape[1], d, size=len(x1))

    else:
        kernel = cupy.ElementwiseKernel(
            'raw T x1, raw T x2, raw Z l1, raw Z l2, Z max_l',
            'raw T d',
            """
            int offset = i * (max_l + 1) * (max_l + 1);
            for(int j = 0; j < l1[i] + 1; j++)
                d[offset + j * (max_l + 1)] = j;
            for(int k = 0; k < l2[i] + 1; k++)
                d[offset + k] = k;
            for(int j = 1; j < l1[i] + 1; j++){
                for(int k = 1; k < l2[i] + 1; k++){
                    int index = offset + j * (max_l + 1) + k;
                    T delta = 0;
                    if(x1[i * max_l + j - 1] != x2[i * max_l + k - 1])
                        delta = 1;
                    d[index] = min(d[index - (max_l + 1)] + 1, min(d[index - 1] + 1, d[index - max_l - 2] + delta));
                }  
            } 
            """,
            name='edit_distance_kernel'
        )
        if l1 is None:
            l1 = cupy.sum((x1 > 0).astype(np.int32), axis=1)
            l2 = cupy.sum((x2 > 0).astype(np.int32), axis=1)
        if x1.shape[1] < 255:
            dtype = np.uint8
        else:
            if x1.shape[1] < 65535:
                dtype = np.uint16
            else:
                dtype = np.uint32
        d = cupy.zeros((x1.shape[0], x1.shape[1] + 1, x1.shape[1] + 1), dtype=dtype)
        kernel(x1.astype(d.dtype), x2.astype(d.dtype), l1.astype(d.dtype), l2.astype(d.dtype), x1.shape[1], d,
               size=len(x1))

    d = d[list(range(len(l1))), l1, l2]
    if normolized:
        d = d - cupy.abs(l1 - l2)
    return d


def dna_to_seed(dna_str):
    # revert seed from the DNA sequence
    s = ''
    for ch in dna_str:
        s += '{0:02b}'.format(int(dna_to_num[ch]))
    return int(s, 2)


def seed_to_dna(seed):
    bin_str = '{:032b}'.format(seed)
    dna_str = ''
    for t in range(0, len(bin_str), 2):
        dna_str += num_to_dna[int(bin_str[t:t + 2], 2)]
    return dna_str


def SSC(indexList, dnaData, filter=False):
    maxClusterIndex = max(indexList, key=lambda k: k[1])
    tags = [index[1] for index in indexList]
    clusters = [[] for _ in range(maxClusterIndex[1])]
    for i, read in enumerate(dnaData):
        clustInd, tag = indexList[i]
        clusters[tag - 1].append(read)
    cnt_corrected = 0
    cnt_detected = 0
    seeds = []
    seedMap = dict()
    clusterMap = dict()
    for i, cluster in enumerate(clusters):
        # Skip empty clusters
        if len(cluster) == 0:
            continue

        # Record the frequencies of seeds
        freq = dict()
        for read in cluster:
            seed = dna_to_seed(read[:16])
            if seed not in freq.keys():
                freq[seed] = 1
            else:
                freq[seed] += 1

        # Majority selection for the center seed
        maxSeed = -1
        maxFreq = 0
        for k in freq.keys():
            if freq[k] > maxFreq:
                maxFreq = freq[k]
                maxSeed = k
        if maxSeed == -1:
            continue

        # Create a mapping
        clusterInd = i + 1
        if maxSeed not in seeds:
            seeds.append(maxSeed)
            seedMap[maxSeed] = clusterInd  # For merging the other clusters
            clusterMap[clusterInd] = clusterInd
        else:
            clusterMap[clusterInd] = seedMap[maxSeed]

    for tag in set(tags):
        if tag not in clusterMap.keys():
            clusterMap[tag] = tag  # Map to itself

    newIndexList = []
    for clustInd, tag in indexList:
        newIndexList.append((clustInd, clusterMap[tag]))
    tags = [index[1] for index in newIndexList]
    return newIndexList


def SSCR(indexList, dnaData, tree_threshold, tree_depth, filter=False):
    maxClusterIndex = max(indexList, key=lambda k: k[1])
    tags = [index[1] for index in indexList]
    clusters = [[] for _ in range(maxClusterIndex[1])]
    for i, read in enumerate(dnaData):
        clustInd, tag = indexList[i]
        clusters[tag - 1].append(read)
    cnt_corrected = 0
    cnt_detected = 0
    seeds = []
    seedMap = dict()
    clusterMap = dict()
    core = []
    tree = Trie()
    clustNum = 0
    for i, cluster in enumerate(clusters):
        # Skip empty clusters
        if len(cluster) == 0:
            continue
        clustNum += 1
        # Record the frequencies of seeds
        freq = dict()
        for read in cluster:
            prefix = read[:tree_depth]
            seed = prefix
            if seed not in freq.keys():
                freq[seed] = 1
            else:
                freq[seed] += 1

        # Majority selection for the center seed
        maxSeed = -1
        maxFreq = 0
        for k in freq.keys():
            if freq[k] > maxFreq:
                maxFreq = freq[k]
                maxSeed = k
        if maxSeed == -1:
            continue

        # Create a mapping
        clusterInd = i + 1

        # Clustering with Tree Structure
        seedStr = maxSeed
        align = tree.fuzz_fin(seedStr, tree_threshold)
        if align[1] < tree_threshold:
            # print('%s Merge To %s Err: %d'%(seedStr[-tree_depth:], readMap[align[0]], sum(align[1])))
            clusterMap[clusterInd] = align[0]  # Merging
            continue

        if core and filter:
            # Prefiltering with edit distance
            distance = all_edit(seedStr, core, tree_depth)
            ind = np.argmin(distance)
            if distance[ind] < tree_threshold:
                clusterMap[clusterInd] = clusterMap[seedMap[core[ind]]]
                continue

        tree.insert(seedStr, clusterInd)
        seedMap[seedStr] = clusterInd  # For merging the other clusters
        clusterMap[clusterInd] = clusterInd
        core.append(seedStr)

    for tag in set(tags):
        if tag not in clusterMap.keys():
            clusterMap[tag] = tag  # Map to itself

    newIndexList = []
    for clustInd, tag in indexList:
        newIndexList.append((clustInd, clusterMap[tag]))
    tags = [index[1] for index in newIndexList]
    return newIndexList


def MSCR(indexList, dnaData, tree_threshold, start_tree_depth, iter=3, reduceSize=2, filter=False):
    indexListMSCR = indexList
    tree_depth = start_tree_depth
    for i in range(iter):
        indexListMSCR = SSCR(indexListMSCR, dnaData, tree_threshold, tree_depth, filter=filter)
        tree_depth -= reduceSize
    return indexListMSCR


"""# Perform SSC clustering"""

tree_threshold = 3
tree_depth = n + 3

print('Single stage clustering')
begin = time.time()
tree = Trie()
indexList = clust(tree, rxs, tree_depth=tree_depth, tree_threshold=tree_threshold)
timeSSC = time.time() - begin
print('Time: ', timeSSC)
acc = computeAccuracy(indexList, gamma)

print('Single stage clustering with index')
begin = time.time()
tree = Trie()
indexListIndex = clust_with_index(tree, rxs, tree_depth=tree_depth, tree_threshold=tree_threshold)
timeSSCIndex = time.time() - begin
print('Time: ', timeSSCIndex)
accIndex = computeAccuracy(indexListIndex, gamma)
# with open('result.txt', 'w') as f:
#     for refInd, clInd in indexList:
#         f.write(str(refInd) + ',' + str(clInd) + '\n')

print('Two stage clustering with index')
begin = time.time()
indexListSSC = SSC(indexListIndex, rxs)
print('Time: ', timeSSCIndex + time.time() - begin)
accSSC = computeAccuracy(indexListSSC, gamma)

# print('Two stage clustering with repeating second stage and index')
# begin = time.time()
# indexListSSCR = SSCR(indexListIndex, rxs, tree_threshold, tree_depth)
# print('Time: ', timeSSCIndex + time.time() - begin)
# accSSCR = computeAccuracy(indexListSSCR, gamma)
#
# print('Two stage clustering with repeating second stage and index (filter)')
# begin = time.time()
# indexListSSCRF = SSCR(indexListIndex, rxs, tree_threshold, tree_depth, filter=True)
# print('Time: ', timeSSCIndex + time.time() - begin)
# accSSCRF = computeAccuracy(indexListSSCRF, gamma)
# print('--------------------------------')
# # MSCR
# begin = time.time()
# indexListMSCR = MSCR(indexList, rxs, tree_threshold, tree_depth, iter=2, reduceSize=2, filter=False)
# print('Time: %f'%(time.time()-begin))
# accMSCR = computeAccuracy(indexListMSCR, gamma)
# print('--------------------------------')
# # MSCRF
# begin = time.time()
# indexListMSCRF = MSCR(indexList, rxs, tree_threshold, tree_depth, iter=2, reduceSize=2, filter=True)
# print('Time: %f'%(time.time()-begin))
# accMSCRF = computeAccuracy(indexListMSCRF, gamma)

"""# Plot"""

# Simulation Error
print(simErr / len(rxs))

# Theoretical Error
print(probFailureClust)

# output graph
plt.figure(dpi=180)
plt.plot([1 - probFailureClust, 1 - probFailureClust], [0, 2], '--', label='Average')
plt.plot([1, 1], [0, 2], '-')
plt.plot(gamma, accMCMC, label='MCMC')
plt.plot(gamma, acc, label='Single-stage')
plt.plot(gamma, accIndex, label='Single-stage with index')
plt.plot(gamma, accSSC, '--', label='Two-stage')
# plt.plot(gamma, accSSCR, '--', label='Two-stage repeat')
# plt.plot(gamma, accSSCRF, '--', label='Two-stage repeat with filter')
# plt.plot(gamma, accMSCR, '--', label='Multi-stage repeat')
# plt.plot(gamma, accMSCRF, '--', label='Multi-stage repeat with filter')
plt.legend()
plt.xlim([0, 1.1])
plt.ylim([0, 1.1])
plt.show(block=False)